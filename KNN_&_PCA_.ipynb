{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJkiLG5Qp228"
      },
      "outputs": [],
      "source": [
        "                         KNN & PCA | Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1:What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "     classification and regression problems?\n",
        "    \n",
        "  - K-Nearest Neighbors (KNN) is a supervised machine learning algorithm used for both classification and regression.\n",
        "* It is a non-parametric method (doesn’t assume any probability   distribution  of data).\n",
        "\n",
        "* It is also a lazy learner, meaning it doesn’t build an explicit model during training; instead, it stores the data and makes predictions only when needed.\n",
        "\n",
        "* KNN in Classification:-\n",
        "\n",
        "Example: Predict whether an email is spam or not spam.\n",
        "\n",
        "Suppose K=5.\n",
        "\n",
        "The algorithm checks the 5 nearest training emails to the new one.\n",
        "\n",
        "If 3 out of 5 neighbors are labeled \"spam\", the prediction = \"spam\".\n",
        "\n",
        "* KNN in Regression:-\n",
        "\n",
        "Example: Predict the price of a house based on its size and location.\n",
        "\n",
        "Suppose K=4.\n",
        "\n",
        "The algorithm finds the 4 nearest houses.\n",
        "\n",
        "The predicted price = average price of these 4 houses."
      ],
      "metadata": {
        "id": "G0PG7O-Tqmk9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 2:What is the Curse of Dimensionality and how does it affect KNN\n",
        "   performance?\n",
        "\n",
        " -  The Curse of Dimensionality refers to the problems that arise when data has too many features (dimensions) compared to the number of samples.\n",
        "\n",
        "As dimensions increase:\n",
        "\n",
        "Data becomes sparse (spread out).\n",
        "\n",
        "Distance metrics (like Euclidean) become less meaningful.\n",
        "\n",
        "Models that rely on distance or density, like KNN, suffer.\n",
        "\n",
        "\n",
        "* Effect on KNN\n",
        "\n",
        "KNN relies heavily on distances to find neighbors.\n",
        "When high dimensions kick in:\n",
        "\n",
        "All points appear almost equally distant, so KNN struggles to identify meaningful neighbors.\n",
        "\n",
        "Predictions become unreliable.\n",
        "\n",
        "Need exponentially more data to maintain accuracy.\n",
        "\n",
        "High risk of overfitting if irrelevant features are included."
      ],
      "metadata": {
        "id": "s7tCrQavspJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 3:What is Principal Component Analysis (PCA)? How is it different from\n",
        "   feature selection?\n",
        "\n",
        " -  PCA is a dimensionality reduction technique that transforms high-dimensional data into a new set of uncorrelated variables called principal components (PCs).\n",
        "\n",
        " * Each principal component is a linear combination of the original features.\n",
        "\n",
        " *  The first PC captures the maximum variance in the data.\n",
        "\n",
        " * The second PC captures the maximum remaining variance (orthogonal to the first), and so on.\n",
        "\n",
        " | **Aspect**     | **PCA (Feature Extraction)**                                                | **Feature Selection**                           |\n",
        "| **Definition** | Creates new features (principal components) by combining original features. | Chooses a subset of the original features.      |\n",
        "| **Type**       | **Feature extraction**                                                      | **Feature selection**                           |\n",
        "| **Output**     | Transformed features (not easily interpretable).                            | Original features (still interpretable).        |\n",
        "| **Goal**       | Preserve maximum variance, reduce redundancy.                               | Keep only the most relevant predictors.         |\n",
        "| **Example**    | From 100 features → create 10 PCs.                                          | From 100 features → pick top 10 important ones. |\n"
      ],
      "metadata": {
        "id": "2FWeG00Wtiqd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 4:What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "   important?\n",
        "\n",
        "  * Eigenvector = direction of variance.\n",
        "\n",
        "  * Eigenvalue = magnitude of variance along that direction.\n",
        "\n",
        "  Why Are They Important in PCA?\n",
        "\n",
        " Eigenvectors:\n",
        "\n",
        "Define the new coordinate system (principal components).\n",
        "\n",
        "Each eigenvector is a direction in feature space along which data varies most.\n",
        "\n",
        " Eigenvalues:\n",
        "\n",
        "Tell us the importance (variance explained) of each principal component.\n",
        "\n",
        "Larger eigenvalue = more information retained.\n",
        "\n",
        "Used to decide how many components to keep (e.g., “keep enough PCs to explain 95% of variance”)."
      ],
      "metadata": {
        "id": "uwguDjl3uuOm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 5:How do KNN and PCA complement each other when applied in a single\n",
        "   pipeline?\n",
        "\n",
        " - Wine dataset has 13 chemical features → not very high-dimensional, but still correlated.\n",
        "\n",
        "KNN suffers when features are correlated / noisy.\n",
        "\n",
        "PCA reduces to fewer uncorrelated principal components, keeping most variance.\n",
        "\n",
        "Combining them → faster computation + potentially better generalization."
      ],
      "metadata": {
        "id": "KMyvJkeMvifU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# 2. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 3. Build pipeline: Scaling → PCA → KNN\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),     # scale features\n",
        "    ('pca', PCA()),                   # PCA step\n",
        "    ('knn', KNeighborsClassifier())   # KNN classifier\n",
        "])\n",
        "\n",
        "# 4. Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'pca__n_components': [2, 5, 7, 10, None],   # try different dimensions\n",
        "    'knn__n_neighbors': [3, 5, 7, 9],           # different K values\n",
        "    'knn__weights': ['uniform', 'distance']     # voting strategy\n",
        "}\n",
        "\n",
        "# 5. Grid search with cross-validation\n",
        "grid = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# 6. Results\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Train Accuracy:\", grid.score(X_train, y_train))\n",
        "print(\"Test Accuracy:\", grid.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZdV57I1wuj_",
        "outputId": "9d81086d-adcf-41a8-f2b0-06c7e7cb5399"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'knn__n_neighbors': 9, 'knn__weights': 'uniform', 'pca__n_components': 7}\n",
            "Train Accuracy: 0.9788732394366197\n",
            "Test Accuracy: 0.9722222222222222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 6:Train a KNN Classifier on the Wine dataset with and without feature\n",
        "   scaling. Compare model accuracy in both cases."
      ],
      "metadata": {
        "id": "AD1h91URwKHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 1. KNN without scaling\n",
        "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scaling.fit(X_train, y_train)\n",
        "acc_no_scaling = knn_no_scaling.score(X_test, y_test)\n",
        "\n",
        "# 2. KNN with scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "knn_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaling.fit(X_train_scaled, y_train)\n",
        "acc_scaling = knn_scaling.score(X_test_scaled, y_test)\n",
        "\n",
        "print(\"Accuracy without scaling:\", acc_no_scaling)\n",
        "print(\"Accuracy with scaling:\", acc_scaling)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7A6Jl8PxYAy",
        "outputId": "7a8df269-048e-4da9-996c-12c49f6b568e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.8055555555555556\n",
            "Accuracy with scaling: 0.9722222222222222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7:Train a PCA model on the Wine dataset and print the explained variance\n",
        "   ratio of each principal component."
      ],
      "metadata": {
        "id": "Um6mVE_uxhuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Explained variance ratio\n",
        "print(\"Explained variance ratio of each principal component:\")\n",
        "for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
        "    print(f\"PC{i+1}: {ratio:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5eDEc32xqmo",
        "outputId": "11bf5baf-6141-42ba-b44b-f78bebce7034"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained variance ratio of each principal component:\n",
            "PC1: 0.3620\n",
            "PC2: 0.1921\n",
            "PC3: 0.1112\n",
            "PC4: 0.0707\n",
            "PC5: 0.0656\n",
            "PC6: 0.0494\n",
            "PC7: 0.0424\n",
            "PC8: 0.0268\n",
            "PC9: 0.0222\n",
            "PC10: 0.0193\n",
            "PC11: 0.0174\n",
            "PC12: 0.0130\n",
            "PC13: 0.0080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2\n",
        "components). Compare the accuracy with the original dataset."
      ],
      "metadata": {
        "id": "cmA3B2B3x3eR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 1. KNN on original scaled dataset\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train_scaled, y_train)\n",
        "acc_original = knn_original.score(X_test_scaled, y_test)\n",
        "\n",
        "# 2. PCA transform (top 2 components)\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "acc_pca = knn_pca.score(X_test_pca, y_test)\n",
        "\n",
        "print(\"Accuracy on original dataset (scaled):\", acc_original)\n",
        "print(\"Accuracy on PCA (2 components):\", acc_pca)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPmcinNIx4lU",
        "outputId": "403299a6-4cf3-4d06-bbfd-87b1cb0ba30c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on original dataset (scaled): 0.9722222222222222\n",
            "Accuracy on PCA (2 components): 0.9166666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9: Train a KNN Classifier with different distance metrics (euclidean,\n",
        "manhattan) on the scaled Wine dataset and compare the results."
      ],
      "metadata": {
        "id": "uTcgqXuHyIz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 2: Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Step 3: Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 4: Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 5: Train KNN with Euclidean distance\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
        "acc_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# Step 6: Train KNN with Manhattan distance\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
        "acc_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "# Step 7: Compare results\n",
        "print(\"KNN with Euclidean Distance Accuracy:\", acc_euclidean)\n",
        "print(\"KNN with Manhattan Distance Accuracy:\", acc_manhattan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urrRsSWPzFp0",
        "outputId": "6967b59d-d2a8-4554-93ca-ad4e72cdfe29"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN with Euclidean Distance Accuracy: 0.9444444444444444\n",
            "KNN with Manhattan Distance Accuracy: 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 10: You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "Explain how you would:\n",
        "● Use PCA to reduce dimensionality\n",
        "● Decide how many components to keep\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "● Evaluate the model\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1. Use PCA to Reduce Dimensionality\n",
        "\n",
        "Gene expression datasets often have thousands of features (genes) but only hundreds of patients (samples) → very high feature-to-sample ratio.\n",
        "\n",
        "This leads to the curse of dimensionality and overfitting if we feed raw features to a model.\n",
        "\n",
        "Principal Component Analysis (PCA) transforms correlated features into a smaller number of uncorrelated components that capture the maximum variance in the data.\n",
        "\n",
        "This reduces noise, removes redundant features, and makes models more generalizable.\n",
        "\n",
        "2. Decide How Many Components to Keep\n",
        "\n",
        "Use the explained variance ratio from PCA.\n",
        "\n",
        "Plot a scree plot (cumulative variance explained vs. number of components).\n",
        "\n",
        "Keep enough components to capture 90–95% of the variance (common in biomedical research).\n",
        "\n",
        "Alternatively, use cross-validation performance (pick the number of components that yields the best classification accuracy without overfitting).\n",
        "\n",
        "3. Use KNN for Classification Post-Dimensionality Reduction\n",
        "\n",
        "After dimensionality reduction, train a K-Nearest Neighbors (KNN) classifier.\n",
        "\n",
        "Why KNN works better after PCA:\n",
        "\n",
        "KNN relies on distance metrics → high dimensions distort distances.\n",
        "\n",
        "PCA reduces dimensions → distances become more meaningful.\n",
        "\n",
        "Choose k (number of neighbors) using GridSearchCV with cross-validation.\n",
        "\n",
        "Try different distance metrics (Euclidean, Manhattan).\n",
        "\n",
        "4. Evaluate the Model\n",
        "\n",
        "Split data into train/test (or use cross-validation if data is small).\n",
        "\n",
        "Use metrics relevant for biomedical data:\n",
        "\n",
        "Accuracy (overall correctness).\n",
        "\n",
        "Precision/Recall/F1-score (important if one cancer type is rare).\n",
        "\n",
        "ROC-AUC (robust for imbalanced classes).\n",
        "\n",
        "Perform stratified cross-validation to ensure all cancer types are represented in each fold.\n",
        "\n",
        "Possibly use nested cross-validation (tuning hyperparameters inside CV) to avoid optimistic bias.\n",
        "\n",
        "5. Justify this Pipeline to Stakeholders\n",
        "\n",
        "Interpretability: PCA compresses thousands of genes into fewer components, making the model easier to understand.\n",
        "\n",
        "Robustness: Reduces noise and prevents overfitting, which is critical given small sample sizes.\n",
        "\n",
        "Generalization: Model trained on PCA-transformed data is more likely to perform well on new patient cohorts.\n",
        "\n",
        "Scalability: This pipeline can adapt when new genes or patients are added.\n",
        "\n",
        "Biomedical validity: PCA often groups genes into biologically meaningful patterns (e.g., co-expressed gene clusters), which can provide insights beyond classification."
      ],
      "metadata": {
        "id": "ecP_HYqkzJE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XsNJdJvMzkEG"
      }
    }
  ]
}